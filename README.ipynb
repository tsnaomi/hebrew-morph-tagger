{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Neural morphological tagging for Hebrew\n",
    "\n",
    "This project presents my early efforts in neural morphological tagging for Modern Hebrew. Here, I define morphological tagging as the attempt to identify morphological properties of words. For instance, we may want a system to learn that היא ('she') is a `3rd-person, singular, feminine pronoun`, identifying the person, number, and gender properties of the word, as well as its part of speech. (In this sense, morphological tagging is the more informative cousin of part-of-speech tagging.) This type of morphological analysis is beneficial for processing morphologically rich languages like Modern Hebrew.\n",
    "\n",
    "This `readme` focuses on the model's implementation in Keras. However, much of the code for this project resides in `data/`, which pre-processes and generates the gold data used below."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The current classification task\n",
    "\n",
    "All Modern Hebrew verbs, nouns, and adjectives are inflected for person (`1st`, `2nd`, `3rd`), number (`singular` and `plural`), and gender (`feminine` and `masculine`), such that they show agreement along these dimensions. Hebrew also inflects verbs for various tenses: `past`, `present`, `future`, `infinitive`, `imperative`, and `beinoni` ([which is similar to a participle](http://www.lrec-conf.org/proceedings/lrec2008/pdf/802_paper.pdf)). In addition, if a Hebrew noun is `definite`, all of its adjectival modifiers must also be marked as `definite`. Since these morphological properties surface ubiquitously in Hebrew, an ideal morphological tagger would detect all of them.\n",
    "\n",
    "One option is to conceive of this task as a *multi-class* classification problem. In multi-class classification, the model attempts to label each word in an utterance with a single label from a set of 3+ labels. For instance, part-of-speech (POS) tagging is usually implemented as a multi-class classifier that predicts a single label for a word from classes like `noun`, `verb`, `adjective`, et cetera. In a neural POS tagger, the final layer is often a *softmax* layer that outputs the probability of the target word being an instance of each part of speech.\n",
    "\n",
    "It is easy to conceive of doing morphological analysis in a similar manner, where we have labels like `def.sg.masc.noun` and `3rd.pl.masc.past.verb`. Each word would therefore have a single correct label and a final *softmax* layer would output the probability of each class being *that* label. However, there is a potential drawback to taking this approach with morphological tagging. Mainly, if we have two labels like `2nd.sg.fem.pronoun` and `2nd.pl.fem.pronoun`, nothing signals to the network that these two labels differ only with respect to number. Nothing explicitly tells the network that they are identical in their person, gender, and part of speech properties.\n",
    "\n",
    "To address this issue, I have implemented the morphological tagger in a *multi-label* fashion, which allows for a word to receive multiple labels (e.g., `noun` *and* `1st-person`). The neural network tries to identify labels for six morphological properties in Hebrew: **part of speech**, **person**, **gender**, **number**, **tense**, and **definiteness**. To capture the many possible values for each property, the output for each word is a 38-dimensional vector, where each cell in the vector corresponds to a particular label:\n",
    "\n",
    "![output vector](./images/output-vec.png)\n",
    "\n",
    "The gold output vectors are \"multi-hot\" encoded, such that a cell is 1 if the label applies to the word and 0 if it doesn't. With this labeling scheme, the gold vectors for `2nd.sg.fem.pronoun` and `2nd.sg.fem.pronoun` differ only in terms of the two cells indicating number (in pink below); they are otherwise identical, reflecting their similar grammatical characteristics. Below, the top vector encodes `2nd.sg.fem.pronoun` and the bottom encodes `2nd.pl.fem.pronoun`:\n",
    "\n",
    "![pronoun vectors](./images/pronoun-vecs.png)\n",
    "\n",
    "Lastly, a multi-label configuration makes it easy to encode the morphology of words that take on multiple labels within a single morphological property. For instance, if the surface form of a verb can be used in the `1st-`, `2nd-`, *and* `3rd-person`, then the gold vector would simply mark 1's in all three of the cells for person."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Model overview\n",
    "\n",
    "Using Keras, I have haphazardly implemented a Long Short-Term Memory (LSTM) neural network for identifying the six Hebrew morphological properties stated above. The architecture for the input and hidden layers is largely inspired by [Lample et al.'s (2016)](https://www.aclweb.org/anthology/N16-1030) paper on character-based LSTMs for named entity recognition, as well as this [Keras tutorial](https://www.depends-on-the-definition.com/lstm-with-char-embeddings-for-ner/) on the same topic.\n",
    "\n",
    "The network takes a sentence as its input and attempts to label each word in the sentence with a vector that encodes the values of the six morphological properties. The model takes advantage of both word embeddings and character embeddings. My hope is that the word embeddings will help the network learn how words behave in the context of other words (which largely reflects part of speech), while the character embeddings will help the network learn their internal morphological structure.\n",
    "\n",
    "Each input sentence is represented by two input vectors. The first vector is the input to a word embedding layer. It assumes a finite dictionary $D^W$ that maps each unique word in the data to a unique index. Thus, if the sentence is of length $n$, then this vector represents the sentence as an $n$-length vector, where each word is represented by its index in $D^C$.\n",
    "\n",
    "Whereas the word embeddinng input is an $n$-dimensional vector, the character-embedding input vector assumes a finite dictionary $D^C$ that maps each unique character in the data to a unique index. If the sentence is of length $n$, then the sentence is represented as an $n$-length vector *of vectors*: Each word is represented by a vector, in which each character in the word is represented by its index in $D^C$.\n",
    "\n",
    "The word embedding input vector enters the network by feeding into a word embedding layer. If the entire vocabulary is of size $V$, then the word embedding layer is a $V \\times d^W$ matrix of parameters, where $d^W$ is the number of hidden units specified for the word embedding. This layer is created using `keras.layers.Embedding`.\n",
    "\n",
    "Similarly, the character embedding input vector enters the network via a character embedding layer. If all of the words in the vocabulary draw exclusively from a set of $C$ characters, then the character embedding layer is a $C \\times d^C$ matrix of parameters, where $d^C$ is the number of hidden units specified for the character embedding. This layer is also created using `keras.layers.Embedding`; however, it is wrapped in a `keras.layers.TimeDistributed` layer that processes the characters in the sentence one word at time (i.e., performing the character embeddings one word at a time).\n",
    "\n",
    "The output of the character embedding layer is passed to a character-level bidirectional LSTM layer (`keras.layers.bidirectional` + `keras.layers.LSTM`) that is once again wrapped in a `TimeDistributed` layer to process the characters one word at a time. The output of this layer is then concatenated with the output of the word embedding layer to create a \"full embedding\". This concatenation is nicely depicted in a figure from [Lample et al. (2016)](https://www.aclweb.org/anthology/N16-1030). Afterwards, the full embedding is passed through another bidirectional LSTM.\n",
    "\n",
    "<img src=\"./images/Lample-figure.png\" alt=\"Lample (2016) figure\" style=\"width: 400px;\">\n",
    "\n",
    "The final layer of the network is a time-distributed `keras.layers.Dense` layer. For each word in the input sentence, it outputs the 38-dimensional vector that encodes the various morphological properties. \n",
    "\n",
    "Since the labels in the output vector are *not* mutually exclusive, it is not appropriate to use *softmax* in the final layer, as *softmax* pits labels against one another for probability mass. Instead, I use the *sigmoid* activation function (shown below), which computes a value for each cell in the output vector independently of all the other cells in the vector.\n",
    "\n",
    "$$\\text{Sigmoid activation function:}\\quad f(x) = \\dfrac{1}{1 + e^{-x}}$$\n",
    "\n",
    "Likewise, during training, the model uses *binary cross entropy* as its loss function to prevent the assignment of one label from influencing the assignment of other labels. *Binary cross entropy* assumes each label has a binary distribution $\\{p, 1-p\\}$; therefore, it calculates the error for each label independently of the next, then sums over the results. Below, $y_i$ is the correct value for label $i$ and $\\hat{y}_i$ is the predicted value for that label.\n",
    "\n",
    "$$\\text{CE}_{\\text{binary}} = -\\sum_i \\big( y_i \\log(\\hat{y}_i) + (1 - y_i)\\log(1-\\hat{y}_i)\\big)$$"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### The data\n",
    "\n",
    "The data in this project draws from a subset of the Israeli [HaAretz](http://www.mila.cs.technion.ac.il/resources_corpora_haaretz.html) daily newspaper corpus, provided by [MILA](http://www.mila.cs.technion.ac.il/index.html). This subset is limited to 199 articles from 1990-1991 that were hand-annotated for morphological structure. From the 199 articles, I extracted 3,527 sentences, which altogether contained 20,806 unique word types across 65,471 tokens. I then performed an 80-10-10 split on the data, where 80% of the sentences were randomly selected for the training set (2,822 sentences), 10% for a development set (353 sentences), and 10% for the test set (352 sentences). For now, the morphological tagger assumes a closed vocabulary."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Playing with the model\n",
    "\n",
    "In `data/data.py`, I created a `Data()` object to extract, format, and navigate the *HaAretz* corpus. Below, I instantiate this object, which gives us access to training, development, and test datasets. Since this project is in its early stages, I test the model here on the development set and refrain from interacting with the test set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "from data import Data  # data is a local module\n",
    "\n",
    "data = Data()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Inspecting the training data, we see that `data.train_X` is a list containing the respective inputs to the word and character embedding layers:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "First sentence vector in word-embedding input:\n",
      "[18772 12467   374 12828 16803 12500 13790  2517 15816  5001 19409  8682\n",
      " 12884 13861  2084 11977 10221     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0     0     0     0     0     0     0     0     0     0\n",
      "     0     0     0]\n",
      "\n",
      "First sentence vector in character-embedding input:\n",
      "[[ 1 15 46 ...  0  0  0]\n",
      " [15 40 35 ...  0  0  0]\n",
      " [17  4 31 ...  0  0  0]\n",
      " ...\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]\n",
      " [ 0  0  0 ...  0  0  0]]\n"
     ]
    }
   ],
   "source": [
    "print('First sentence vector in word-embedding input:')\n",
    "print(data.train_X[0][0])\n",
    "print()\n",
    "print('First sentence vector in character-embedding input:')\n",
    "print(data.train_X[1][0])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To accommodate variable length words and sentences, the input vectors are zero-padded. All of the sentence vectors have been set to the length of the longest sentence in the corpus (`data.max_sent_len`). Likewise, all of the word vectors have been set to the length of the longest word in the corpus (`data.max_word_len`)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Maximum sentence length: \t\t 75\n",
      "Maximum word length: \t\t\t 16\n",
      "\n",
      "\u001b[1mTRAINING\u001b[0m\n",
      "Number of examples: \t\t\t 2822\n",
      "Shape of word embeddings input: \t (2822, 75)\n",
      "Shape of character embeddings input: \t (2822, 75, 16)\n",
      "Shape of output labels: \t\t (2822, 75, 38)\n",
      "\n",
      "\u001b[1mDEV\u001b[0m\n",
      "Number of examples: \t\t\t 353\n",
      "Shape of word embeddings input: \t (353, 75)\n",
      "Shape of character embeddings input: \t (353, 75, 16)\n",
      "Shape of output labels: \t\t (353, 75, 38)\n",
      "\n",
      "\u001b[1mTEST\u001b[0m\n",
      "Number of examples: \t\t\t 352\n",
      "Shape of word embeddings input: \t (352, 75)\n",
      "Shape of character embeddings input: \t (352, 75, 16)\n",
      "Shape of output labels: \t\t (352, 75, 38)\n"
     ]
    }
   ],
   "source": [
    "print('Maximum sentence length:', '\\t\\t', data.max_sent_len)\n",
    "print('Maximum word length:', '\\t\\t\\t', data.max_word_len)\n",
    "\n",
    "print('\\n\\033[1mTRAINING\\033[0m')\n",
    "print('Number of examples:', '\\t\\t\\t', len(data.train_X[0]))\n",
    "print('Shape of word embeddings input:', '\\t', data.train_X[0].shape)\n",
    "print('Shape of character embeddings input:', '\\t', data.train_X[1].shape)\n",
    "print('Shape of output labels:', '\\t\\t', data.train_Y.shape)\n",
    "\n",
    "print('\\n\\033[1mDEV\\033[0m')\n",
    "print('Number of examples:', '\\t\\t\\t', len(data.dev_X[0]))\n",
    "print('Shape of word embeddings input:', '\\t', data.dev_X[0].shape)\n",
    "print('Shape of character embeddings input:', '\\t', data.dev_X[1].shape)\n",
    "print('Shape of output labels:', '\\t\\t', data.dev_Y.shape)\n",
    "\n",
    "print('\\n\\033[1mTEST\\033[0m')\n",
    "print('Number of examples:', '\\t\\t\\t', len(data.test_X[0]))\n",
    "print('Shape of word embeddings input:', '\\t', data.test_X[0].shape)\n",
    "print('Shape of character embeddings input:', '\\t', data.test_X[1].shape)\n",
    "print('Shape of output labels:', '\\t\\t', data.test_Y.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The morphological tagger is implemented in `nn.py` as `MorphTagger`. Below, I use `MorphTagger` to load a previously trained model. However, if the tagger is passed a filename for a nonexistent file, it will train and save a new model. (For now, I have hard-coded arbitrary values in `nn.py` for different parameters required by the network, such as batch_size and dropout.)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_2 (InputLayer)            (None, 75, 16)       0                                            \n",
      "__________________________________________________________________________________________________\n",
      "input_1 (InputLayer)            (None, 75)           0                                            \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_1 (TimeDistrib (None, 75, 16, 200)  11200       input_2[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 75, 150)      3120750     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_2 (TimeDistrib (None, 75, 200)      240800      time_distributed_1[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 75, 350)      0           embedding_1[0][0]                \n",
      "                                                                 time_distributed_2[0][0]         \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_2 (Bidirectional) (None, 75, 200)      360800      concatenate_1[0][0]              \n",
      "__________________________________________________________________________________________________\n",
      "time_distributed_3 (TimeDistrib (None, 75, 38)       7638        bidirectional_2[0][0]            \n",
      "==================================================================================================\n",
      "Total params: 3,741,188\n",
      "Trainable params: 3,741,188\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "from nn import MorphTagger\n",
    "\n",
    "MODEL_FN = './models/morph_tagger.h5'\n",
    "\n",
    "model = MorphTagger(\n",
    "    model_fn=MODEL_FN,\n",
    "    train_X=data.train_X,\n",
    "    train_Y=data.train_Y,\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "With that, lets generate some predictions with `MorphTagger` and view the model's performance:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "353/353 [==============================] - 8s 23ms/step\n",
      "\n",
      "\u001b[1mPrediction accuracies given 6252 words across 353 sentences\u001b[0m:\n",
      "╒══════════════════╤═══════════╕\n",
      "│ ALL-OR-NOTHING   │   0.69002 │\n",
      "╞══════════════════╪═══════════╡\n",
      "│ POS              │   0.81942 │\n",
      "├──────────────────┼───────────┤\n",
      "│ PERSON           │   0.93138 │\n",
      "├──────────────────┼───────────┤\n",
      "│ GENDER           │   0.90739 │\n",
      "├──────────────────┼───────────┤\n",
      "│ NUMBER           │   0.88564 │\n",
      "├──────────────────┼───────────┤\n",
      "│ TENSE            │   0.94386 │\n",
      "├──────────────────┼───────────┤\n",
      "│ DEFINITENESS     │   0.86644 │\n",
      "╘══════════════════╧═══════════╛\n"
     ]
    }
   ],
   "source": [
    "predictions = model.predict(data.dev_X)\n",
    "model.acc(predictions, data.dev_Y)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "To assess the quality of the model's predictions, I imbued the `Data()` object  with a `decipher()` method that will return a human-readable string given an output vector; e.g., see the gold vector for a `3rd-person, singular, feminine pronoun` below. The `decipher()` method attributes a label like *singular* to a word if the model gives that label a value greater or equal to a specified threshold (e.g., `threshold=0.5`). With that said, it will always assign the part of speech with the highest value, regardless of whether that value meets the threshold, since all tokens inherently belong to a POS class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'pronoun 3 feminine singular'"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import numpy as np\n",
    "\n",
    "label_vec = np.array([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0,\n",
    "     0, 0, 1, 1, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 0, 0])  # היא (she)\n",
    "\n",
    "data.decipher(label_vec, threshold=0.5)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Below, I randomly select a sentence from the predictions and show the predicted and gold labels side-by-side for each word in the sentence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mמבחינה\u001b[0m\n",
      "noun feminine singular indefinite (predicted)\n",
      "noun feminine singular indefinite (gold)\n",
      "\n",
      "\u001b[1mאידיאולוגית\u001b[0m\n",
      "adjective feminine indefinite (predicted)\n",
      "adjective feminine singular indefinite (gold)\n",
      "\n",
      "\u001b[1m,\u001b[0m\n",
      "punctuation (predicted)\n",
      "punctuation (gold)\n",
      "\n",
      "\u001b[1mעם\u001b[0m\n",
      "preposition (predicted)\n",
      "mwe (gold)\n",
      "\n",
      "\u001b[1mזאת\u001b[0m\n",
      "pronoun indefinite (predicted)\n",
      "mwe indefinite (gold)\n",
      "\n",
      "\u001b[1m,\u001b[0m\n",
      "punctuation (predicted)\n",
      "punctuation (gold)\n",
      "\n",
      "\u001b[1mאנו\u001b[0m\n",
      "pronoun feminine masculine plural indefinite (predicted)\n",
      "pronoun 1 feminine masculine plural indefinite (gold)\n",
      "\n",
      "\u001b[1mמאמינים\u001b[0m\n",
      "participle 1 2 3 masculine plural indefinite (predicted)\n",
      "participle 1 2 3 masculine plural indefinite (gold)\n",
      "\n",
      "\u001b[1mכיום\u001b[0m\n",
      "adverb (predicted)\n",
      "adverb (gold)\n",
      "\n",
      "\u001b[1mבסוציאליזם\u001b[0m\n",
      "noun masculine singular definite (predicted)\n",
      "noun masculine singular indefinite (gold)\n",
      "\n",
      "\u001b[1mיותר\u001b[0m\n",
      "adverb (predicted)\n",
      "adverb (gold)\n",
      "\n",
      "\u001b[1mמאי\u001b[0m\n",
      "mwe (predicted)\n",
      "mwe (gold)\n",
      "\n",
      "\u001b[1m-\u001b[0m\n",
      "punctuation (predicted)\n",
      "punctuation (gold)\n",
      "\n",
      "\u001b[1mפעם\u001b[0m\n",
      "noun indefinite (predicted)\n",
      "mwe (gold)\n",
      "\n",
      "\u001b[1m.\u001b[0m\n",
      "punctuation (predicted)\n",
      "punctuation (gold)\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# randomly select a sentence index\n",
    "i = np.random.randint(0, high=len(data.dev_Y), size=1)[0]\n",
    "\n",
    "# compare the predictions against the gold labels for the randomly selected sentence\n",
    "for word_idx, pred, gold in zip(data.dev_X[0][i], predictions[i], data.dev_Y[i]):\n",
    "\n",
    "    # if the word is not a pad token\n",
    "    if word_idx > 1:\n",
    "        print('\\033[1m%s\\033[0m' % data.idx_word[word_idx])  # the word in Hebrew\n",
    "        print(data.decipher(pred), '(predicted)')            # the prediction\n",
    "        print(data.decipher(gold), '(gold)\\n')               # the actual/gold label"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
